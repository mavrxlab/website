<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>MA{VR}X Lab</title>
<link>https://mavrxlab.org/news.html</link>
<atom:link href="https://mavrxlab.org/news.xml" rel="self" type="application/rss+xml"/>
<description>Official site for the University of Arizona&#39;s MA{VR}X Lab</description>
<generator>quarto-1.2.280</generator>
<lastBuildDate>Mon, 19 Dec 2022 07:00:00 GMT</lastBuildDate>
<item>
  <title>Welcome to the new mavericks!</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2022-12-19-new-folks/index.html</link>
  <description><![CDATA[ 




<p><img src="https://mavrxlab.org/news/2022-12-19-new-folks/featured.jpeg" class="img-fluid"></p>
<p>I’m thrilled to welcome the newest members of the MA{VR}X Lab for the spring 2023 semester!</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mavrxlab.org/news/2022-12-19-new-folks/wildcat.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Wilbur</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mavrxlab.org/news/2022-12-19-new-folks/wildcat.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Wilbur</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mavrxlab.org/news/2022-12-19-new-folks/wildcat.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Wilbur</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mavrxlab.org/news/2022-12-19-new-folks/wildcat.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Wilbur</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{straight2022,
  author = {Ryan Straight},
  title = {Welcome to the New Mavericks!},
  date = {2022-12-19},
  url = {https://mavrxlab.org/news/2022-12-19-new-folks},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2022" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2022. <span>“Welcome to the New Mavericks!”</span>
December 19, 2022. <a href="https://mavrxlab.org/news/2022-12-19-new-folks">https://mavrxlab.org/news/2022-12-19-new-folks</a>.
</div></div></section></div> ]]></description>
  <category>meta</category>
  <guid>https://mavrxlab.org/news/2022-12-19-new-folks/index.html</guid>
  <pubDate>Mon, 19 Dec 2022 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2022-12-19-new-folks/featured.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A Philosophy of Technology and Education in the Metaverse</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2022-11-12-ed3-postphenom/index.html</link>
  <description><![CDATA[ 




<p>The director of the MA{VR}X Lab, Dr.&nbsp;Ryan Straight, will be giving a talk at the first annual Ed3 Conference.</p>
<p><strong>The conference is run on Eastern Daylight Time (UTC-04).</strong></p>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<p>Feel free to <a href="deck/">enjoy the slide deck</a> made with <a href="https://quarto.org">Quarto</a>.</p>
</section>
<section id="full-abstract" class="level2">
<h2 class="anchored" data-anchor-id="full-abstract">Full Abstract</h2>
<p>Experience and education in the metaverse is, without fail, mediated by technology at some point. As we move further into a pervasive and ubiquitous metaversal landscape, it is crucial we consider these experiences as experiences, filters, and knowledge-generating moments in time that, while virtual, have real-world impacts. In this session, Dr.&nbsp;Ryan Straight, honors professor and lab director at the University of Arizona’s College of Applied Science and Technology will discuss the ways in which understanding these mediations leads to better teaching, better learning, and a more equitable experience for all.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{straight2022,
  author = {Ryan Straight},
  title = {A {Philosophy} of {Technology} and {Education} in the
    {Metaverse}},
  date = {2022-11-13},
  url = {https://www.ed3dao.com/ed3con-day3-schedule#:~:text=A%20Philosophy%20of%20Technology%20and%20Education%20in%20the%20Metaverse},
  langid = {en},
  abstract = {Experience and education in the metaverse is, without
    fail, mediated by technology at some point. As we move further into
    a pervasive and ubiquitous metaversal landscape, it is crucial we
    consider these experiences as experiences, filters, and
    knowledge-generating moments in time that, while virtual, have
    real-world impacts. In this session, Dr. Ryan Straight, honors
    professor and lab director at the University of Arizona’s College of
    Applied Science and Technology will discuss the ways in which
    understanding these mediations leads to better teaching, better
    learning, and a more equitable experience for all.}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2022" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2022. <span>“A Philosophy of Technology and Education in
the Metaverse.”</span> Talk. <a href="https://www.ed3dao.com/ed3con-day3-schedule#:~:text=A%20Philosophy%20of%20Technology%20and%20Education%20in%20the%20Metaverse">https://www.ed3dao.com/ed3con-day3-schedule#:~:text=A%20Philosophy%20of%20Technology%20and%20Education%20in%20the%20Metaverse</a>.
</div></div></section></div> ]]></description>
  <guid>https://mavrxlab.org/news/2022-11-12-ed3-postphenom/index.html</guid>
  <pubDate>Sun, 13 Nov 2022 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2022-11-12-ed3-postphenom/featured.png" medium="image" type="image/png" height="78" width="144"/>
</item>
<item>
  <title>Web3 and Education: An Optimistic Primer on Online Learning’s Blockchain-based Future</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2022-11-01-ac22-ed3/index.html</link>
  <description><![CDATA[ 




<p>The director of the MA{VR}X Lab, Dr.&nbsp;Ryan Straight will be hosting a panel at this year’s Online Learning Consortium’s <strong>Accelerate</strong> conference.</p>
<p><strong>The conference is run on Eastern Daylight Time (UTC-04).</strong></p>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<p>Feel free to <a href="deck/">enjoy the slide deck</a> made with <a href="https://quarto.org">Quarto</a>.</p>
</section>
<section id="full-abstract" class="level2">
<h2 class="anchored" data-anchor-id="full-abstract">Full Abstract</h2>
<p>With the shifting fundamental, underlying structures of how online learning is designed and delivered, it is important for educators, researchers, instructional designers, even leaders and students to understand just what this means and how to prepare. There is widespread confusion and concern over the (what many see as inevitable) move into a decentralized, transparent internet and what that could mean for online education if not approached with care, consideration, and intention. For example, how does one approach designing instruction for decentralized delivery and facilitation? What does that even mean? Isn’t a wallet just the same as a portfolio? What happens when there is no centralized, gated vault of knowledge, but rather a democratized outpouring of it? And how do we address authenticity and accountability? This discussion seeks to move beyond simply the excitement of the potentials of Web3 and address possible concerns and criticisms, as well. The Web3 space has a very steep learning curve, something this panel hopes to flatten at least in the online learning space. In short, the goal is to, to quote one of our panelists, “Discern signal from noise.”</p>
<p>The panel will have a set number of groundwork-setting topics and questions but will be open to questions from the audience. In fact, this is highly encouraged. As per usual, attendees will be able to submit questions throughout the discussion and the moderator will do their best to get to them.</p>
<p>Given the relatively short timeframe for the session and the likelihood of being able to answer everyone’s questions during, attendees of this session will be invited to a growing, vibrant community of educators, researchers, and leaders in the Web3 space with a focus on not just “doing Web3,” but doing it right, where the conversation can continue and grow. Attendees will also leave with a working understanding of what these concepts actually mean and the ability to engage authentically in the conversations that will inevitably arise. Finally, attendees will leave with their (likely) first true engagement with the Web3 ecosystem: an NFT! (No annoyed monkeys, we promise. And if you a) don’t know what that is, or b) are skeptical about it, this is definitely the panel for you!)</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{straight2022,
  author = {Ryan Straight},
  title = {Web3 and {Education:} {An} {Optimistic} {Primer} on {Online}
    {Learning’s} {Blockchain-based} {Future}},
  date = {2022-11-01},
  eventdate = {2022-04-13},
  url = {https://onlinelearningconsortium.org/olc-accelerate-2022-session-page/?session=11670},
  langid = {en},
  abstract = {The internet is changing and online learning will
    necessarily change with it. Terms like “crypto,” “blockchain,”
    “NFT,” “DAO,” and “Web3” are possibly not entirely new to you, but
    do you know what to expect when these stop being theoretical and
    become infused into the very bedrock of online learning? Join our
    panel of experts and educators to help answer questions like “What
    problem does this solve?,” “What value does this add?,” “How does it
    work?,” and “What does it even do?”}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2022" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2022. <span>“Web3 and Education: An Optimistic Primer on
Online Learning’s Blockchain-Based Future.”</span> Talk. <a href="https://onlinelearningconsortium.org/olc-accelerate-2022-session-page/?session=11670">https://onlinelearningconsortium.org/olc-accelerate-2022-session-page/?session=11670</a>.
</div></div></section></div> ]]></description>
  <guid>https://mavrxlab.org/news/2022-11-01-ac22-ed3/index.html</guid>
  <pubDate>Tue, 01 Nov 2022 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2022-11-01-ac22-ed3/featured.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Submitting a Project to the Lab</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2022-02-10-project-submission/index.html</link>
  <description><![CDATA[ 




<p>The entire reason the MA{VR}X Lab exists is to produce human-focused and applied research products and services. To that end, we encourage project submissions, so we can help however possible, whether that’s providing consultation, advising, equipment, software, or just a place to work.</p>
<p><i class="fa-solid fa-question-circle" aria-label="question-circle"></i> What kinds of projects does the lab accept?</p>
<p>We are currently open to pretty much anything but are focusing on VR and AR projects as that is what the lab is currently equipped for.</p>
<p><i class="fa-solid fa-question-circle" aria-label="question-circle"></i> Does my project idea have to be fleshed out or funded?</p>
<p>Neither! Obviously, if your project is funded and you want to partner with the lab, that’s fantastic. If you’re just in the initial ideation stage, that’s also great. We’re here to help.</p>
<p><i class="fa-solid fa-question-circle" aria-label="question-circle"></i> I’m a student in (insert class or independent study here) and have a project idea that involves (VR/AR/XR/etc). Can I work with the lab on this?</p>
<p>Absolutely! For students that aren’t working for or in the lab, this is actually ideal. Work with your instructor before submitting a project.</p>
<p><i class="fa-solid fa-question-circle" aria-label="question-circle"></i> How do I submit a project to the lab?</p>
<p>Use <a href="https://forms.office.com/r/ABPfYchMPD">this form</a> to submit a project. Submissions are currently only open to those in the University of Arizona community. If you are outside the institution, you’ll need your UArizona sponsor/collaborator to submit the project. Alternately, scan the provided QR code.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mavrxlab.org/news/2022-02-10-project-submission/project-submission-form.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Project submission form QR code</figcaption><p></p>
</figure>
</div>
</div></div><p><i class="fa-solid fa-question-circle" aria-label="question-circle"></i> I’ve submitted a project. What’s next?</p>
<p>Upon submitting the project, one of the lab staff will verify it and add it to <a href="https://emailarizona.sharepoint.com/sites/CAST-MAVRX-Lab/Lists/Project%20Tracker/">the project tracker</a>. You are then free to schedule time in the lab, request support, and do whatever else you need to do for your project. You will be assigned a lab worker as a point-of-contact that can help you schedule time, work with equipment, and so on.</p>
<p><i class="fa-solid fa-question-circle" aria-label="question-circle"></i> Where’s the project tracker?</p>
<p>The <a href="https://emailarizona.sharepoint.com/sites/CAST-MAVRX-Lab/Lists/Project%20Tracker/">project tracker is a Microsoft List</a> that’s tied to our lab’s Microsoft Team. Once you’ve been added to the Team, you’ll have access to and find the Project Tracker tab in the <code>project-updates</code> channel. If you’ve submitted a project and it’s been approved, you’ll receive an email with a link to edit your own projects within that List.</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{straight2022,
  author = {Ryan Straight},
  title = {Submitting a {Project} to the {Lab}},
  date = {2022-02-10},
  url = {https://mavrxlab.org/news/2022-02-10-project-submission},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2022" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2022. <span>“Submitting a Project to the Lab.”</span>
February 10, 2022. <a href="https://mavrxlab.org/news/2022-02-10-project-submission">https://mavrxlab.org/news/2022-02-10-project-submission</a>.
</div></div></section></div> ]]></description>
  <category>meta</category>
  <guid>https://mavrxlab.org/news/2022-02-10-project-submission/index.html</guid>
  <pubDate>Thu, 10 Feb 2022 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2022-02-10-project-submission/featured.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>New Year, New(sletter)!</title>
  <dc:creator>Ryan Straight</dc:creator>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2022-01-04-new-year-new-sletter/index.html</link>
  <description><![CDATA[ 




<p><img src="https://mavrxlab.org/news/2022-01-04-new-year-new-sletter/featured.jpeg" class="img-fluid"></p>
<p>Wishing you could have more MA{VR}X Lab content? You’re in luck! We’ve begun a weekly newsletter! You can subscribe to <strong>MetaMeta</strong> by checking out the <a href="../../newsletter">Newsletter</a> tab or visiting https://www.getrevue.co/profile/metameta</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{straight2022,
  author = {Ryan Straight and Ryan Straight},
  title = {New {Year,} {New(sletter)!}},
  date = {2022-01-04},
  url = {https://mavrxlab.org/news/2022-01-04-new-year-new-sletter},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2022" class="csl-entry quarto-appendix-citeas">
Ryan Straight, and Ryan Straight. 2022. <span>“New Year,
New(sletter)!”</span> January 4, 2022. <a href="https://mavrxlab.org/news/2022-01-04-new-year-new-sletter">https://mavrxlab.org/news/2022-01-04-new-year-new-sletter</a>.
</div></div></section></div> ]]></description>
  <category>meta</category>
  <guid>https://mavrxlab.org/news/2022-01-04-new-year-new-sletter/index.html</guid>
  <pubDate>Tue, 04 Jan 2022 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2022-01-04-new-year-new-sletter/featured.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Simulation, Immersion, and Gamification</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2022-01-03-olc-digital-literacy/index.html</link>
  <description><![CDATA[ 




<p>Dr.&nbsp;Straight will be sitting on a UNESCO-supported webinar to talk extended reality, artificial intelligence, and online education. This is part 3 of 5 in a series of webinars entitled “Digital Literacy &amp; AI.” The topic is “Simulation, Immersion, and Gamification.” This event is hosted by the Online Learning Consortium and organized by Dr.&nbsp;Ben Scragg.</p>
<section id="takeaways" class="level2">
<h2 class="anchored" data-anchor-id="takeaways">Takeaways</h2>
<ul>
<li>Explore emerging technologies that are driving global innovation for teaching and learning through simulation, immersion, and gamification</li>
<li>Identify opportunities and technologies to implement innovative solutions across a spectrum of access, cost, and openness.</li>
<li>Gain insights on research, adoption and implementation strategies for scaling innovative technologies across systems and institutions.</li>
</ul>
</section>
<section id="intended-audience" class="level2">
<h2 class="anchored" data-anchor-id="intended-audience">Intended audience</h2>
<p>Higher Education Administrators, Higher Education Faculty, K-12 Administrators, K-12 Faculty, Online Education Directors, Instructional and Educational Technologists, Learning Experience Designers, Education Policy Makers, Educational Futurists, International Community</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{straight2022,
  author = {Ryan Straight},
  title = {Simulation, {Immersion,} and {Gamification}},
  date = {2022-01-03},
  eventdate = {2022-01-10},
  url = {https://onlinelearningconsortium.org/webinar/digital-literacy-ai-webinar-series-3-simulation-immersion-and-gamification/},
  langid = {en},
  abstract = {As the real and virtual worlds continue to intertwine, we
    are reorienting ourselves to the notion of serious play, and to the
    value of simulation, immersion, and game-based learning. In this
    webinar, our expert panelists will explore the emergent
    possibilities and exciting teaching, learning, and doing taking
    place through immersive, virtual experiences.}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2022" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2022. <span>“Simulation, Immersion, and
Gamification.”</span> Talk. <a href="https://onlinelearningconsortium.org/webinar/digital-literacy-ai-webinar-series-3-simulation-immersion-and-gamification/">https://onlinelearningconsortium.org/webinar/digital-literacy-ai-webinar-series-3-simulation-immersion-and-gamification/</a>.
</div></div></section></div> ]]></description>
  <guid>https://mavrxlab.org/news/2022-01-03-olc-digital-literacy/index.html</guid>
  <pubDate>Mon, 03 Jan 2022 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2022-01-03-olc-digital-literacy/featured.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Karu: Introducing the Metaversal Library for the Future of Immersive Learning</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2021-12-14-in22/index.html</link>
  <description><![CDATA[ 




<p>The director of the MA{VR}X Lab, Dr.&nbsp;Ryan Straight will be presenting a conference talk at the Online Learning Consortium’s <strong>Innovate 2022</strong> conference this in Dallas, April 13. This is an elaboration on and update for the <a href="https://mavrxlab.org/event/vwf21/">Virtual Worlds Forum</a> talk he gave on the same project.</p>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<p>Feel free to <a href="deck/">enjoy the slide deck</a> made with <a href="https://quarto.org">Quarto</a> for a very cool, totally Powerpoint-free experience! (Please be aware that, during the talk, the slide deck linked here will <em>advance automatically</em> as I advance mine. <a href="https://quarto.org/docs/presentations/revealjs/presenting.html#multiplex">This is a feature, not a bug!</a>)</p>
</section>
<section id="full-abstract" class="level2">
<h2 class="anchored" data-anchor-id="full-abstract">Full Abstract</h2>
<p>What is extended reality? Who lives and works in the metaverse? You have a great idea for a VR project, but with whom do you connect? And how? Why should you even consider this extended reality space to begin with?</p>
<p>These are questions we ask as a working research group of faculty and students at a major R1 university. To answer these, we envision a project known as <strong>Karu</strong> (formerly XRpedia). This platform is a place where people, projects, locations, services, even funding and employment opportunities exist to connect researchers, developers, instructors, and especially students in ways that will drive innovation in the metaverse. One main goal of <strong>Karu</strong> is to be user- and learner-focused, in that everything within it–hardware, companies, you name it–always comes back to the people. The importance of finding a way to make these connections within the metaverse, itself, cannot be overstated. Doing it in such a way that is accessible, secure, dependable, and sustainable is a driving force behind <strong>Karu</strong>.</p>
<p>What is the “metaverse” and how does it apply to online learning? It encompasses virtually (pun intended) all experiences within the space of online learning: virtual and augmented reality, persistent digital spaces like discussion forums or Zoom rooms, even the entirety of the internet and every way in which it is utilized.</p>
<p>It is our hope that, as we develop and continue to shape <strong>Karu</strong>, attendees to the session–experts in online learning that they are–will provide great food for thought, both for us as the developers and for one another. This session will be primarily an introduction to the project, as well as a look at the roadmap for where it can and will go.</p>
</section>
<section id="relevant-references" class="level2">
<h2 class="anchored" data-anchor-id="relevant-references">Relevant References</h2>
<div class="csl-bib-body" style="line-height: 2; margin-left: 2em; text-indent:-2em;">
<div class="csl-entry">
Basilotta-Gómez-Pablos, V., Matarranz, M., Casado-Aranda, L.-A., &amp; Otto, A. (2022). Teachers’ digital competencies in higher education: A systematic literature review. <i>International Journal of Educational Technology in Higher Education</i>, <i>19</i>(1), 8. <a href="https://doi.org/10.1186/s41239-021-00312-8">https://doi.org/10.1186/s41239-021-00312-8</a>
</div>
<div class="csl-entry">
Bursali, H., &amp; Yilmaz, R. M. (2019). Effect of augmented reality applications on secondary school students’ reading comprehension and learning permanency. <i>Computers in Human Behavior</i>, <i>95</i>, 126–135. <a href="https://doi.org/10.1016/j.chb.2019.01.035">https://doi.org/10.1016/j.chb.2019.01.035</a>
</div>
<div class="csl-entry">
Campos-Mesa, M.-C., Castañeda-Vázquez, C., DelCastillo-Andrés, Ó., &amp; González-Campos, G. (2022). Augmented Reality and the Flipped Classroom—A Comparative Analysis of University Student Motivation in Semi-Presence-Based Education Due to COVID-19: A Pilot Study. <i>Sustainability</i>, <i>14</i>(4), 2319. <a href="https://doi.org/10.3390/su14042319">https://doi.org/10.3390/su14042319</a>
</div>
<div class="csl-entry">
Chiang, F.-K., Shang, X., &amp; Qiao, L. (2022). Augmented reality in vocational training: A systematic review of research and applications. <i>Computers in Human Behavior</i>, <i>129</i>, 107125. <a href="https://doi.org/10.1016/j.chb.2021.107125">https://doi.org/10.1016/j.chb.2021.107125</a>
</div>
<div class="csl-entry">
Di, X., &amp; Zheng, X. (2022). A meta-analysis of the impact of virtual technologies on students’ spatial ability. <i>Educational Technology Research and Development</i>, <i>70</i>(1), 73–98. <a href="https://doi.org/10.1007/s11423-022-10082-3">https://doi.org/10.1007/s11423-022-10082-3</a>
</div>
<div class="csl-entry">
Garzón, J., &amp; Acevedo, J. (2019). Meta-analysis of the impact of Augmented Reality on students’ learning gains. <i>Educational Research Review</i>, <i>27</i>, 244–260. <a href="https://doi.org/10.1016/j.edurev.2019.04.001">https://doi.org/10.1016/j.edurev.2019.04.001</a>
</div>
<div class="csl-entry">
Howard, M. C., &amp; Gutworth, M. B. (2020). A meta-analysis of virtual reality training programs for social skill development. <i>Computers &amp; Education</i>, <i>144</i>, 103707. <a href="https://doi.org/10.1016/j.compedu.2019.103707">https://doi.org/10.1016/j.compedu.2019.103707</a>
</div>
<div class="csl-entry">
Ibáñez, M. B., Uriarte Portillo, A., Zatarain Cabada, R., &amp; Barrón, M. L. (2020). Impact of augmented reality technology on academic achievement and motivation of students from public and private Mexican schools. A case study in a middle-school geometry course. <i>Computers &amp; Education</i>, <i>145</i>, 103734. <a href="https://doi.org/10.1016/j.compedu.2019.103734">https://doi.org/10.1016/j.compedu.2019.103734</a>
</div>
<div class="csl-entry">
Kimmons, R., &amp; Rosenberg, J. M. (2022). Trends and Topics in Educational Technology, 2022 Edition. <i>TechTrends</i>, <i>66</i>(2), 134–140. <a href="https://doi.org/10.1007/s11528-022-00713-0">https://doi.org/10.1007/s11528-022-00713-0</a>
</div>
<div class="csl-entry">
Kyaw, B. M., Saxena, N., Posadzki, P., Vseteckova, J., Nikolaou, C. K., George, P. P., Divakar, U., Masiello, I., Kononowicz, A. A., Zary, N., &amp; Tudor Car, L. (2019). Virtual Reality for Health Professions Education: Systematic Review and Meta-Analysis by the Digital Health Education Collaboration. <i>Journal of Medical Internet Research</i>, <i>21</i>(1), e12959. <a href="https://doi.org/10.2196/12959">https://doi.org/10.2196/12959</a>
</div>
<div class="csl-entry">
Li, F., Wang, X., He, X., Cheng, L., &amp; Wang, Y. (2021). How augmented reality affected academic achievement in K-12 education – a meta-analysis and thematic-analysis. <i>Interactive Learning Environments</i>, 1–19. <a href="https://doi.org/10.1080/10494820.2021.2012810">https://doi.org/10.1080/10494820.2021.2012810</a>
</div>
<div class="csl-entry">
Merchant, Z., Goetz, E. T., Cifuentes, L., Keeney-Kennicutt, W., &amp; Davis, T. J. (2014). Effectiveness of virtual reality-based instruction on students’ learning outcomes in K-12 and higher education: A meta-analysis. <i>Computers &amp; Education</i>, <i>70</i>, 29–40. <a href="https://doi.org/10.1016/j.compedu.2013.07.033">https://doi.org/10.1016/j.compedu.2013.07.033</a>
</div>
<div class="csl-entry">
Tsai, C.-Y., &amp; Lai, Y.-C. (2022). Design and Validation of an Augmented Reality Teaching System for Primary Logic Programming Education. <i>Sensors</i>, <i>22</i>(1), 389. <a href="https://doi.org/10.3390/s22010389">https://doi.org/10.3390/s22010389</a>
</div>
<div class="csl-entry">
Villena-Taranilla, R., Tirado-Olivares, S., Cózar-Gutiérrez, R., &amp; González-Calero, J. A. (2022). Effects of virtual reality on learning outcomes in K-6 education: A meta-analysis. <i>Educational Research Review</i>, <i>35</i>, 100434. <a href="https://doi.org/10.1016/j.edurev.2022.100434">https://doi.org/10.1016/j.edurev.2022.100434</a>
</div>
<div class="csl-entry">
Wu, B., Yu, X., &amp; Gu, X. (2020). Effectiveness of immersive virtual reality using head‐mounted displays on learning performance: A meta‐analysis. <i>British Journal of Educational Technology</i>, <i>51</i>(6), 1991–2005. <a href="https://doi.org/10.1111/bjet.13023">https://doi.org/10.1111/bjet.13023</a>
</div>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{straight2021,
  author = {Ryan Straight},
  title = {Karu: {Introducing} the {Metaversal} {Library} for the
    {Future} of {Immersive} {Learning}},
  date = {2021-12-14},
  eventdate = {2022-04-13},
  url = {https://onlinelearningconsortium.org/olc-innovate-2022-session-page/?session=10765},
  langid = {en},
  abstract = {As education moves inexorably toward fully immersive,
    connected experiences, it is important to ensure openness,
    transparency, and collaboration. Karu (formerly XRpedia, the
    eXtended Reality encyclopedia) is an ongoing project that seeks to
    do precisely that. This session will introduce the project, its
    motivation, and the roadmap ahead.}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2021" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2021. <span>“Karu: Introducing the Metaversal Library for
the Future of Immersive Learning.”</span> Talk. <a href="https://onlinelearningconsortium.org/olc-innovate-2022-session-page/?session=10765">https://onlinelearningconsortium.org/olc-innovate-2022-session-page/?session=10765</a>.
</div></div></section></div> ]]></description>
  <guid>https://mavrxlab.org/news/2021-12-14-in22/index.html</guid>
  <pubDate>Tue, 14 Dec 2021 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2021-12-14-in22/featured.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Business at Twilight: Come visit the lab!</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2021-08-11-business-at-twilight-2021/index.html</link>
  <description><![CDATA[ 




<p><img src="https://mavrxlab.org/news/2021-08-11-business-at-twilight-2021/featured.jpeg" class="img-fluid"></p>
<p>For those attending, we’ll also have a <a href="https://app.poap.xyz/admin/events/mavrx-business-at-twilight-2021-2021">POAP</a> NFT for fans of digital collections. Stop by the lab to get the details!</p>
<div class="cell" data-labe="poap">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">knitr<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">include_graphics</span>(<span class="st" style="color: #20794D;">"https://assets.poap.xyz/mavrx-business-at-twilight-2021-2021-logo-1636475019436.png"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://mavrxlab.org/news/2021-08-11-business-at-twilight-2021/https:/assets.poap.xyz/mavrx-business-at-twilight-2021-2021-logo-1636475019436.png" class="img-fluid"></p>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{straight2021,
  author = {Ryan Straight},
  title = {Business at {Twilight:} {Come} Visit the Lab!},
  date = {2021-11-18},
  url = {https://mavrxlab.org/news/2021-08-11-business-at-twilight-2021},
  langid = {en},
  abstract = {The University of Arizona’s College of Applied Science and
    Technology is partnering with the Sierra Vista Chamber of Commerce
    to host an evening on our campus for building relationships through
    networking with chamber members, University of Arizona faculty and
    staff, and other community leaders. The MA\{VR\}X Lab is
    participating, exhibiting a range of augmented, virtual, and
    extended reality experiences. Visitors will have the opportunity to
    fly over Mt. Fuji in Japan, get up close and personal with a blue
    whale, hold Jupiter in their hands, paint graffiti in thin air, and
    box in Egypt.}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2021" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2021. <span>“Business at Twilight: Come Visit the
Lab!”</span> November 18, 2021. <a href="https://mavrxlab.org/news/2021-08-11-business-at-twilight-2021">https://mavrxlab.org/news/2021-08-11-business-at-twilight-2021</a>.
</div></div></section></div> ]]></description>
  <guid>https://mavrxlab.org/news/2021-08-11-business-at-twilight-2021/index.html</guid>
  <pubDate>Thu, 18 Nov 2021 17:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2021-08-11-business-at-twilight-2021/featured.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Virtually Dining Under the Stars</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2021-dine_under_the_stars/index.html</link>
  <description><![CDATA[ 




<p><img src="https://mavrxlab.org/news/2021-dine_under_the_stars/featured.png" class="img-fluid"></p>
<p><em>Dine Under the Stars</em> is an annual fundraising event put on by the <a href="https://www.universitysouthfoundation.com/dineunderthestars">University South Foundation</a> to raise money for CAST students. This year’s theme is “Reach For the Stars.” From the Foundation:</p>
<blockquote class="blockquote">
<p>Enjoy dinner provided by Texas Roadhouse, Indochine Family Restaurant, and Bobke’s, live music by Desert Fever, stargazing through the Patterson Observatory Telescope, a great selection of silent &amp; live auction items and more.</p>
<p>Since 2011 the Foundation has given out $675,000 in scholarships to support University of Arizona students at the Douglas and Sierra Vista campuses. It is only through the generosity of our local communities that this has been achieved.</p>
<p>Special thanks to this year’s sponsors who make the event possible: Pioneer Title Agency, ACE Hardware, Alma Dolores International Dance Centre, Cherry Creek Radio, Borowiec &amp; Borowiec, PC – Attorneys at Law, Cardinal Pointe Financial Group, Cochise County Sheriff’s Assist Team, Desert Eagle Security, Grasshopper Landscaping, Groth Rutherford Properties LLC, Herald/Review Media, Huachuca Astronomy Club, ISC Consulting Group, KKYZ – 101.7 FM, Elsie and Paul MacMillan, New Frontier Animal Medical Center, Rainey Pain &amp; Performance, Rutherford Diversified Properties LLC, Sierra Toyota, and Tierra Antigua – Maria Juvera.</p>
<p>Together, we are making a difference. Please join us as we come together with others passionate about education and the University of Arizona. For more information please call: 520-458-8278 x2129.</p>
</blockquote>
<p>What does this have to do with the lab, you might ask? Why, space, of course!</p>
<p>As the Patterson Observatory is also owned by the University South Foundation and <a href="https://mavrxlab.org/post/patterson-observatory-3d-tour/">the lab completed the 3D scan of the observatory</a> recently, we’ve teamed up again to provide attendees of the Dine Under the Stars fundraiser with a virtual reality accompaniment. Donors were not only given access to the 20” telescope in the observatory, but also to the <a href="https://spaceengine.org/">Space Engine</a> universe simulation software in the lab.</p>
<p>Here’s <a href="https://www.reddit.com/r/spaceengine/comments/pykuca/such_a_view_i_just_love_it_could_stare_for_hours/">a recording by Reddit user <code>BerkeA111</code> of just how amazing this experience is</a>:</p>
<div class="cell">
<div class="cell-output-display">
{{% youtube "Y7BPlWE1Aww" %}}
</div>
</div>
<p>How mind-blowing is that? You can find the original video <a href="https://drive.google.com/u/1/uc?id=1yGIMCC26Im39LlxH9ZhLsocbjxmDBJBJ&amp;export=download">on Google Drive</a>.</p>
<p>What else will we have? A way for folks to pick up a star or a planet and turn it around in their hands using <a href="https://mergeedu.com/cube">the Merge Cube</a>.</p>
<p>[::: {.cell-output-display} <img src="https://mavrxlab.org/news/2021-dine_under_the_stars/https:/blog.airsquirrels.com/hs-fs/hubfs/Blog%20Images/B327%20-%20AR-VR%20Christine%20Danhoff%203/teacher-screen-mirroring-merge-cube.gif?width=900&amp;name=teacher-screen-mirroring-merge-cube.gif" class="img-fluid"> :::</p>
<p>](https://blog.airsquirrels.com/edtech/how-to-teach-virtual-and-augmented-reality-merge-cube-explorer)</p>
<p>We’ll also be providing <a href="https://poap.gallery/event/8980">a POAP NFT</a> to anyone that stops by.</p>
<p>[::: {.cell-output-display} <img src="https://mavrxlab.org/news/2021-dine_under_the_stars/poap.png" class="img-fluid" width="320"> :::</p>
<p>](https://poap.gallery/event/8980)</p>
<p>See you there!</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{straight2021,
  author = {Ryan Straight},
  title = {Virtually {Dining} {Under} the {Stars}},
  date = {2021-10-02},
  url = {https://mavrxlab.org/news/2021-dine_under_the_stars},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2021" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2021. <span>“Virtually Dining Under the Stars.”</span>
October 2, 2021. <a href="https://mavrxlab.org/news/2021-dine_under_the_stars">https://mavrxlab.org/news/2021-dine_under_the_stars</a>.
</div></div></section></div> ]]></description>
  <guid>https://mavrxlab.org/news/2021-dine_under_the_stars/index.html</guid>
  <pubDate>Sun, 03 Oct 2021 01:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2021-dine_under_the_stars/featured.png" medium="image" type="image/png" height="72" width="144"/>
</item>
<item>
  <title>Patterson Observatory 3D tour</title>
  <dc:creator>Ryan Straight</dc:creator>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2021-09-02-patterson-observatory-3d-tour/index.html</link>
  <description><![CDATA[ 




<p><img src="https://mavrxlab.org/news/2021-09-02-patterson-observatory-3d-tour/featured.jpg" class="img-fluid"></p>
<p>In August of this year, we had the pleasure of spending the afternoon in the <a href="https://www.universitysouthfoundation.com/patterson-observatory">Patterson Observatory in Sierra Vista</a>, doing a Matterport 3D scan of this incredible space.</p>
<blockquote class="blockquote">
<p>The Patterson Observatory is owned by the University South Foundation, Inc.&nbsp;and operated by volunteers from the Huachuca Astronomy Club (HAC). It is named for David Patterson who was a founding member of HAC and a major Foundation donor.</p>
<p>The observatory opened in September 2004 and consists of a fork-mounted f/8.1, 20-inch Ritchey Chretien reflector under a 16-foot dome. The observatory is a NASA Space Place community partner and the focal point for astronomy outreach and education in Southeast Arizona.</p>
</blockquote>
<p>Patterson Observatory is a tremendously educational and entertaining experience, and providing this fully immersive, 3D educational experience goes toward making this experience wider and more accessible. You can enjoy exploring this amazing location below (put on your VR headset for the best experience possible!):</p>
<iframe width="853" height="480" src="https://my.matterport.com/show/?m=dpHpMg7Lpjj" frameborder="0" allowfullscreen="" allow="xr-spatial-tracking">
</iframe>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{straight2021,
  author = {Ryan Straight and Ryan Straight},
  title = {Patterson {Observatory} {3D} Tour},
  date = {2021-09-20},
  url = {https://mavrxlab.org/news/2021-09-02-patterson-observatory-3d-tour},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2021" class="csl-entry quarto-appendix-citeas">
Ryan Straight, and Ryan Straight. 2021. <span>“Patterson Observatory 3D
Tour.”</span> September 20, 2021. <a href="https://mavrxlab.org/news/2021-09-02-patterson-observatory-3d-tour">https://mavrxlab.org/news/2021-09-02-patterson-observatory-3d-tour</a>.
</div></div></section></div> ]]></description>
  <category>3D scanning</category>
  <guid>https://mavrxlab.org/news/2021-09-02-patterson-observatory-3d-tour/index.html</guid>
  <pubDate>Mon, 20 Sep 2021 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2021-09-02-patterson-observatory-3d-tour/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Introducing Proof of Attendance Protocol (POAP)</title>
  <dc:creator>Ryan Straight</dc:creator>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2021-08-10-poap/index.html</link>
  <description><![CDATA[ 




<p><img src="https://mavrxlab.org/news/2021-08-10-poap/featured.jpeg" class="img-fluid"></p>
<p>I’m strictly of the mindset that, when we talk about “virtual reality,” we need to expand our thinking beyond just headsets and floating signs. (This is one of the reasons I dubbed this lab the <strong>MA{VR}X Lab</strong>, to demonstrate that we should always consider all angles in tandem.) With the recent popularity of the <em>metaverse</em> concept (thanks, Zuck), a variety of issues crop up, not least of all being proof of identity and experience. Enter <strong>POAP</strong>.</p>
<p><strong>Update 2022-10-12</strong>: <em>much</em> has changed in the POAP ecosystem since this article was initially published. The growth has been exponential and the quality standards and processes have increased and changed considerably. Great news, all around!</p>
<section id="what-is-poap" class="level2">
<h2 class="anchored" data-anchor-id="what-is-poap">What is POAP?</h2>
<p>Pronounced “PO-app” (like the <em>po’ boy</em> sandwich), POAP is an acronym for <em>Proof Of Attendance Protocol</em>. Check out the <a href="https://intercom.help/poap/en/articles/5820491-what-is-poap">What is POAP: Explanation of the POAP ecosystem and components, and how they all fit together</a> article for a fantastic rundown.</p>
<div style="width:100%;height:0;padding-bottom:56%;position:relative;">
<iframe src="https://giphy.com/embed/SwZroLWy2ifdBKjXP5" width="100%" height="100%" style="position:absolute" frameborder="0" class="giphy-embed" allowfullscreen="">
</iframe>
</div>
<p>
<a href="https://giphy.com/gifs/netflix-poe-altered-carbon-alteredcarbon-SwZroLWy2ifdBKjXP5">A <b>Poe app</b>. Get it? Via GIPHY</a>
</p>
<p>Essentially, it works like this:</p>
<ol type="1">
<li>You’re planning an event or something similar. This could be a physical event or virtual. There are many use-cases, some of which are <em>very</em> creative.</li>
<li>You create a POAP event for this, listing location, dates, how long POAPs can be minted (more on this soon), and a few other details.</li>
<li>This generates codes that you provide to people that have somehow engaged with your event. (There are other ways to get POAPs to people. See the article linked above for more on this.)</li>
<li>These codes give your participants POAPs that exist on the blockchain, where they can collect them as NFTs. (Think: permanent, portable badges.)</li>
</ol>
<p>And that’s it! These are NFTs (non-fungible tokens) that folks can carry with them wherever they go. They are unique, they are immutable, and have at least personal value (though they are <em>technically</em> possible to sell but, even then, there’s a paper trail, as it were, on the blockchain of where it came from and who has had it), though having <em>professional</em> value is not unrealistic, as these can be awarded for professional development experiences just as much as concerts or art gallery openings.</p>
<p>For example, I made a <code>UAAppComp</code> “test” event just to give it a whirl. I received the links to the POAPs shortly after, and this is what it looks like for someone who was given a link (or scanned a QR code at your event):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mavrxlab.org/news/2021-08-10-poap/images/uaappcomptest.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">POAP example</figcaption><p></p>
</figure>
</div>
<p>At this point the attendee puts in their Ethereum address or email and, voila, they have a POAP!</p>
<p><strong>Note</strong>: when you create or claim an NFT, this process is called “minting.” With POAPs, you can specific just when participants can do this. If you only wanted folks to be able to get their POAP during an event, sure, lock it down. If you want them to be able to mint that proof for a week after the event (or, say the event is a week long), knock yourself out. Don’t get too bogged down in the language.</p>
</section>
<section id="our-first-poap" class="level2">
<h2 class="anchored" data-anchor-id="our-first-poap">Our first POAP</h2>
<p>I’m currently working on creating the lab’s very first POAP (beyond the test run above). For those that have helped the lab get set up and running, provided support, or “got in on the ground floor,” as I like to think of it, there will be a special POAP just for them. As time goes on and we have more events either virtually or physically, more POAPs will become available.</p>
<p>Can you tell I’m excited?</p>
<p>When we actually start creating these and getting folks involved, I’ll probably do another post with a step-by-step on how to get them, even if you have no blockchain knowledge or experience. Watch this space!</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{straight2021,
  author = {Ryan Straight and Ryan Straight},
  title = {Introducing {Proof} of {Attendance} {Protocol} {(POAP)}},
  date = {2021-08-11},
  url = {https://mavrxlab.org/news/2021-08-10-poap},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2021" class="csl-entry quarto-appendix-citeas">
Ryan Straight, and Ryan Straight. 2021. <span>“Introducing Proof of
Attendance Protocol (POAP).”</span> August 11, 2021. <a href="https://mavrxlab.org/news/2021-08-10-poap">https://mavrxlab.org/news/2021-08-10-poap</a>.
</div></div></section></div> ]]></description>
  <category>POAP</category>
  <guid>https://mavrxlab.org/news/2021-08-10-poap/index.html</guid>
  <pubDate>Wed, 11 Aug 2021 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2021-08-10-poap/featured.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Dr. Straight at Virtual Worlds Forum</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2021-08-11-vwf21/index.html</link>
  <description><![CDATA[ 




<p>Dr.&nbsp;Straight gave an introduction to a secret project at the 2021 Virtual Worlds Forum, an invite-only event to engage government, higher education, industry, and the military in all things XR.</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{straight2021,
  author = {Ryan Straight},
  title = {Dr. {Straight} at {Virtual} {Worlds} {Forum}},
  date = {2021-08-11},
  eventdate = {2021-11-04},
  url = {https://azcast.arizona.edu/events/733-business-twilight},
  langid = {en},
  abstract = {The theme of this event was *Into the Metaverse*. As more
    of the “big-hitters” in the extended reality (XR) Industry express
    their vision, interest, and investment in the next disruptive
    platform, the “Metaverse” has become a common expression of what
    that will be. How that will impact the way people learn, train,
    live, work, socialize, travel, and entertain within the next decade
    is important for us to understand, prepare for, and leverage.}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2021" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2021. <span>“Dr. Straight at Virtual Worlds
Forum.”</span> Talk. <a href="https://azcast.arizona.edu/events/733-business-twilight">https://azcast.arizona.edu/events/733-business-twilight</a>.
</div></div></section></div> ]]></description>
  <guid>https://mavrxlab.org/news/2021-08-11-vwf21/index.html</guid>
  <pubDate>Wed, 11 Aug 2021 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2021-08-11-vwf21/featured.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Technological Mediation: A Postphenomenology Primer for Instructors, Designers, and More</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2021-08-10-ac21/index.html</link>
  <description><![CDATA[ 




<p>The director of the MA{VR}X Lab, Dr.&nbsp;Ryan Straight will be presenting a conference talk at the Online Learning Consortium’s <strong>Accelerate</strong> conference this September. While not obviously related to the lab, Dr.&nbsp;Straight explains:</p>
<blockquote class="blockquote">
<p>Understanding the empirical frameworks within which the study of our personal experiences are mitigated through technology cannot be understated. This talk will explore exactly this.</p>
</blockquote>
<p><strong>The conference is run on Eastern Daylight Time (UTC-04).</strong></p>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<p>Feel free to <a href="https://ryanstraight.github.io/ac21-postphenom/index.html">enjoy the slide deck</a> made with <a href="https://slides.yihui.org/xaringan/#1">xaringan</a> and <a href="https://pkg.garrickadenbuie.com/xaringanExtra/#/">xaringanExtra</a>.</p>
</section>
<section id="full-abstract" class="level2">
<h2 class="anchored" data-anchor-id="full-abstract">Full Abstract</h2>
<p>Pacemakers, smart mirrors, microscopes, pencils, iPads, cars, air conditioners, VR headsets, and the LMS. What do these have in common? Postphenomenology!</p>
<p>Wait, come back! Don’t let the word put you off: “postphenomenology,” while obnoxiously long, is one of the great secret weapons in the philosophy of technology toolbelt and one that instructors and instructional designers alike can benefit from wielding. Generally speaking, at its core, postphenomenology is an empirical method of studying how technology mediates, for better or worse, our experience of the world. Our concern, however, is the way technology mediates the online learning experience in particular. For example, how does the experience of online learning change when the hardware changes from laptop to PC to smartphone to tablet to virtual reality headset? How about from live instruction via webcam to spatially recorded events? What about the physical environment the student is in? If the student is wearing headphones or not? What if there are accessibility considerations? All these variables, all these complicating, mediating factors, can be addressed with the application of postphenomenological analysis.</p>
<p>In a variation on the traditional postphenomenological understanding of technological mediation, we will trace the experience through the learner, into the technology, the design of the instruction, to the content, and back again, identifying each of the “enigma points” at which that mediation occurs and necessarily can divert what is actually being experienced from what and how it was intended. Like designing a user or learning experience, you can’t design the experience, itself, just design for a particular experience.</p>
<p>This is, for many, likely a whole new methodology for thinking about and developing instructional design using the postphenomenological framework. In applying specific concepts like “technic relations,” “transparency,” and “multistability” to the notion of learning through and via technology, it’s possible to shine an exploratory light on the mediating and complicating connections required to—successfully and with fidelity—bring educational content to an online learner. Exploring how these technic relations and mediations work is key, but also understanding when and how they fail can be even more illuminating.</p>
<p>In this session, we will explore what this postphenomenological framework is, what it tells us about how technology mediates learning, and how you can apply this in solving your own instructional dilemmas. You are encouraged to bring a challenge you’re facing and apply the postphenomenological framework to it and, if nothing else, break it down and view it in a new light. You may just walk away with a brand new understanding about your relationship with the world, not to mention with learning.</p>
</section>
<section id="resources-and-references" class="level2">
<h2 class="anchored" data-anchor-id="resources-and-references">Resources and References</h2>
<p>Aagaard, J. (2015). Media multitasking, attention, and distraction: A critical discussion. Phenomenology and the Cognitive Sciences, 14(4), 885–896. https://doi.org/10.1007/s11097-014-9375-x</p>
<p>Aagaard, J. (2016). Introducing postphenomenological research: A brief and selective sketch of phenomenological research methods. International Journal of Qualitative Studies in Education, 30(6), 519–533. https://doi.org/10.1080/09518398.2016.1263884</p>
<p>Jensen, M. M., &amp; Aagaard, J. (2018). A postphenomenological method for HCI research. Proceedings of the 30th Australian Conference on Computer-Human Interaction - OzCHI ’18, 242–251. https://doi.org/10.1145/3292147.3292170</p>
<p>Adams, C., &amp; Turville, J. (2018). Doing Postphenomenology in Education. In Postphenomenological Methodologies: New Ways in Mediating Techno-Human Relationships (pp.&nbsp;3–25). Lexington Books. echnology (2nd ed., pp.&nbsp;539–560). Wiley-Blackwell.</p>
<p>Ihde, D. (2009). Postphenomenology and Technoscience: The Peking University Lectures. SUNY Press.</p>
<p>Ihde, D. (2008). Introduction: Postphenomenological research. Human Studies, 31(1), 1–9. https://doi.org/10.1007/s10746-007-9077-2</p>
<p>Ihde, D. (2012). Experimental Phenomenology: Multistabilities (2nd ed.). State University of New York Press.</p>
<p>Ihde, D. (2014). A Phenomenology of Technics. In R. C. Scharff &amp; V. Dusek (Eds.), Philosophy of T</p>
<p>Irving, L. (2016). Virtual Worlds as Pedagogical Places: Experiences of Higher Education Academics [Doctoral dissertation, Deakin University]. http://dro.deakin.edu.au/view/DU:30088534</p>
<p>Tripathi, A. K. (2015). Postphenomenological investigations of technological experience. AI and Society, 30(2), 199–205. https://doi.org/10.1007/s00146-014-0575-2</p>
<p>Wellner, G. (2021). The Zoom-bie Student and the Lecturer: Reflections on Teaching and Learning with Zoom. Techné: Research in Philosophy and Technology, 25(1), 1–25.</p>
<p>Verbeek, P. P. (2008). Cyborg intentionality: Rethinking the phenomenology of human-technology relations. Phenomenology and the Cognitive Sciences, 7(3), 387–395. https://doi.org/10.1007/s11097-008-9099-x</p>
<p>Vindenes, J., &amp; Wasson, B. (2021). A Postphenomenological Framework for Studying User Experience of Immersive Virtual Reality. Frontiers in Virtual Reality, 2, 656423. https://doi.org/10.3389/frvir.2021.656423</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{straight2021,
  author = {Ryan Straight},
  title = {Technological {Mediation:} {A} {Postphenomenology} {Primer}
    for {Instructors,} {Designers,} and {More}},
  date = {2021-08-10},
  eventdate = {2021-09-21},
  url = {https://onlinelearningconsortium.org/olc-accelerate-2021-session-page/?session=10264},
  langid = {en},
  abstract = {\_Postphenomenology\_ isn’t just the biggest word at the
    conference; it’s also one of the most useful tools for understanding
    the deep and complex web of relationships between instructors,
    designers, students, content, and technology. If you’ve felt like
    something’s missing in designing online learning, this just may be
    it.}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2021" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2021. <span>“Technological Mediation: A Postphenomenology
Primer for Instructors, Designers, and More.”</span> Talk. <a href="https://onlinelearningconsortium.org/olc-accelerate-2021-session-page/?session=10264">https://onlinelearningconsortium.org/olc-accelerate-2021-session-page/?session=10264</a>.
</div></div></section></div> ]]></description>
  <guid>https://mavrxlab.org/news/2021-08-10-ac21/index.html</guid>
  <pubDate>Tue, 10 Aug 2021 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2021-08-10-ac21/featured.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>What is the MA{VR}X Lab?</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2021-07-21-what-is-the-mavrx-lab/index.html</link>
  <description><![CDATA[ 




<p>The <strong>M</strong>ixed <strong>A</strong>ugmented <strong>V</strong>i<strong>R</strong>tual e<strong>X</strong>tended (Reality) Laboratory (referred to as the MAVRX Lab, pronounced <em>mavericks</em>) is space in which we drive innovation through alternative reality modalities and research. It came about in early 2021 as an entity, though its physical location had existed for some time. MA{VR}X Lab is intended as a collaborate effort, meant to be interdisciplinary and reside in a place of praxis and innovation.</p>
<section id="what-is-praxis" class="level3">
<h3 class="anchored" data-anchor-id="what-is-praxis">What is praxis?</h3>
<p>Praxis as we refer to it can be defined as:</p>
<blockquote class="blockquote">
<p>the process by which a theory, lesson, or skill is enacted, embodied, or realized [and] the act of engaging, applying, exercising, realizing, or practicing ideas<sup>1</sup>.</p>
</blockquote>
<p>It is our goal to apply this to bleeding-edge innovation in alternate reality spaces.</p>
</section>
<section id="mission" class="level2">
<h2 class="anchored" data-anchor-id="mission">Mission</h2>
<p>The primary mission of the MA{VR}X Lab is to develop human-focused ideas through technology, transparency, and care, with a focus on extending our reality using technology and evidence-based methodology.</p>
</section>
<section id="vision" class="level2">
<h2 class="anchored" data-anchor-id="vision">Vision</h2>
<p>Manifested in the nexus of technology research, evidence-based pedagogy, and boundary-pushing ideas, the MA{VR}X Lab will act as a space for chances to be taken, brilliance to be realized, and people to come together.</p>
</section>
<section id="values" class="level2">
<h2 class="anchored" data-anchor-id="values">Values</h2>
<p>The MA{VR}X Lab’s vision is one of transparency, openness, and optimism. At all possible times, we believe knowledge should be not just presented publicly but developed there, as well. We are humans. We make mistakes and we want to make them in public so others may learn. We make those on our way to create great things and help build a better future for everyone.</p>
<hr>
<p>1: <a href="https://en.wikipedia.org/wiki/Praxis_(process)">Praxis (process) - Wikipedia</a></p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{straight2021,
  author = {Ryan Straight},
  title = {What Is the {MA\{VR\}X} {Lab?}},
  date = {2021-07-21},
  url = {https://mavrxlab.org/news/2021-07-21-what-is-the-mavrx-lab},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2021" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2021. <span>“What Is the MA{VR}X Lab?”</span> July 21,
2021. <a href="https://mavrxlab.org/news/2021-07-21-what-is-the-mavrx-lab">https://mavrxlab.org/news/2021-07-21-what-is-the-mavrx-lab</a>.
</div></div></section></div> ]]></description>
  <category>meta</category>
  <guid>https://mavrxlab.org/news/2021-07-21-what-is-the-mavrx-lab/index.html</guid>
  <pubDate>Wed, 21 Jul 2021 07:00:00 GMT</pubDate>
  <media:content url="https://mavrxlab.org/news/2021-07-21-what-is-the-mavrx-lab/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Capturing Groth Hall</title>
  <dc:creator>Ryan Straight</dc:creator>
  <link>https://mavrxlab.org/news/2021-06-14-capturing-groth-hall/index.html</link>
  <description><![CDATA[ 




<p>I’m collaborating with the VR lab at the <a href="https://home.army.mil/huachuca/index.php">Fort Huachuca US Army Base</a> on a few different projects but this was one we could bust out pretty quick just to get something on the books. So, the MA{VR}X Lab’s very first official project: the Groth Hall renovation scan!</p>
<section id="groth-hall" class="level2">
<h2 class="anchored" data-anchor-id="groth-hall">Groth Hall</h2>
<p>Currently the administrative center of the Sierra Vista campus, Groth Hall was opened in 1993 but (I believe) the Sierra Vista campus began construction in 1988.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://www.universitysouthfoundation.com/history"><img src="https://mavrxlab.org/news/2021-06-14-capturing-groth-hall/https:/static.wixstatic.com/media/edd9e8_5a0ae5bbe5f3481797651309eaeb90fe.jpg/v1/fill/w_552,h_311,fp_0.44_0.42,q_90/edd9e8_5a0ae5bbe5f3481797651309eaeb90fe.jpg" class="img-fluid figure-img"></a></p>
<p></p><figcaption class="figure-caption">Groth Hall, Sierra Vista campus, opened 1993</figcaption><p></p>
</figure>
</div>
<p>So, while the building does host a couple classrooms and the <a href="https://www.universitysouthfoundation.com/">University South Foundation’s office</a>, it is mostly for CAST administrative staff offices, along with the folks from the <a href="https://transfer.arizona.edu/near-you-network">Near You (formerly Distance) Network</a>.</p>
</section>
<section id="the-hardware" class="level2">
<h2 class="anchored" data-anchor-id="the-hardware">The Hardware</h2>
<p>Along with an iPad to run the Matterport application that captures the scans, we were using two cameras:</p>
<section id="matterport-pro2" class="level3">
<h3 class="anchored" data-anchor-id="matterport-pro2">Matterport Pro2</h3>
<p>Scanning physical spaces has come a long way. Case in point, the <a href="https://matterport.com/cameras/pro2-3D-camera">Matterport Pro2 3D Camera</a>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://matterport.com/cameras/pro2-3D-camera"><img src="https://mavrxlab.org/news/2021-06-14-capturing-groth-hall/https:/matterport.com/sites/default/files/images/Pro2Hero.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>This beast comes in at <strong>134</strong> megapixels.</p>
<p>The photography website Adorama has a great introductory video for the camera:</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/mGa-uiKRLGg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="leica-blk360" class="level3">
<h3 class="anchored" data-anchor-id="leica-blk360">Leica BLK360</h3>
<p>We were also using this monster:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mavrxlab.org/news/2021-06-14-capturing-groth-hall/https:/www.sccssurvey.co.uk/media/catalog/product/cache/1/image/800x/040ec09b1e35df139433887a97daa66f/l/e/leica_blk360_led.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Leica BLK360</figcaption><p></p>
</figure>
</div>
<p>The <a href="https://leica-geosystems.com/products/laser-scanners/scanners/blk360">Leica BLK360 Imaging Laser Scanner</a> is really <a href="https://shop.leica-geosystems.com/learn/reality-capture/blk360">a surveying instrument</a> more than a traditional 360 degree camera (despite the product number).</p>
<blockquote class="blockquote">
<p>The BLK360 is a compact imaging laser scanner that uses a 360° laser distance meter and high definition panoramic imaging to create a 3D point cloud of the space around it.</p>
</blockquote>
<p>The Leica takes between 3 and 5 minutes to complete a scan depending on a few factors but we found it averaged between 4 and 4 1/2. Compared to the Matterport, that means it takes anywhere between 6 and 8 times as long to complete the scan.</p>
<p>So why use the Leica instead of the Matterport? I’ll give you a hint:</p>
<div style="width:100%;height:0;padding-bottom:94%;position:relative;">
<iframe src="https://giphy.com/embed/5TBlqBtLtrzUc" width="100%" height="100%" style="position:absolute" frameborder="0" class="giphy-embed" allowfullscreen="">
</iframe>
</div>
<p>
<a href="https://giphy.com/gifs/sun-desert-raccoon-5TBlqBtLtrzUc">via GIPHY</a>
</p>
<p>First of all, the mid-day sun is very confusing for cameras like the Matterport, as they can have a difficult time making connections with previous scan points. So, in those cases, the Leica–which is a lidar scanner, remember–works just fine in full sun.</p>
<p>Mostly.</p>
<p>Yesterday, when we were doing the Groth Hall scans–which had been planned for weeks, mind–the temperature in Sierra Vista wasn’t far behind that of Tucson, <a href="https://kvoa.com/weather/2021/06/14/excessive-heat-through-saturday-and-a-slight-chance-for-storms/">which reached 115°F (46.1°C)</a>. The Leica camera is black. We were scanning outside. Yeah.</p>
<p>Needless to say, while we did get nearly 300 scans done, we just missed out on the very last few because of the sun. Those will get completed at a later date as they’re not crucial to the building scan.</p>
<p>So, now it’s a matter of getting the 3D model uploaded and ready to work with. Here’s a little preview:</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mavrxlab.org/news/2021-06-14-capturing-groth-hall/IMG_0001-screen.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Groth Hall Matterport scan</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Watch this space for more updates on the project!</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{straight2021,
  author = {Ryan Straight},
  title = {Capturing {Groth} {Hall}},
  date = {2021-06-14},
  url = {https://mavrxlab.org/news/2021-06-14-capturing-groth-hall},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-straight2021" class="csl-entry quarto-appendix-citeas">
Ryan Straight. 2021. <span>“Capturing Groth Hall.”</span> June 14, 2021.
<a href="https://mavrxlab.org/news/2021-06-14-capturing-groth-hall">https://mavrxlab.org/news/2021-06-14-capturing-groth-hall</a>.
</div></div></section></div> ]]></description>
  <category>3D scanning</category>
  <guid>https://mavrxlab.org/news/2021-06-14-capturing-groth-hall/index.html</guid>
  <pubDate>Mon, 14 Jun 2021 07:00:00 GMT</pubDate>
  <media:content url="https://static.wixstatic.com/media/edd9e8_5a0ae5bbe5f3481797651309eaeb90fe.jpg/v1/fill/w_552,h_311,fp_0.44_0.42,q_90/edd9e8_5a0ae5bbe5f3481797651309eaeb90fe.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
