<!DOCTYPE html>
<html lang="en"><head>
<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/tabby.min.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/quarto-contrib/qrcodejs-v1.0.0/qrcode.js"></script><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.58">

  <meta name="author" content=" Ryan Straight, Ph.D ">
  <title>OUR POSTHUMAN LEARNERS</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="assets/style.css">
  <link href="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-attribution/attribution.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title"><p><img align="center" width="75%" src="assets/lab.png"></p></h1>
  <p class="subtitle"></p><p><span style="font-size: 125%">OUR POSTHUMAN LEARNERS</span><br><strong>AI</strong> AND THE FUTURE OF EDUCATION</p><p></p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<br><strong>Ryan Straight, Ph.D</strong><br> 
</div>
        <p class="quarto-title-affiliation">
            College of Applied Science and Technology<br>University of Arizona
          </p>
    </div>
</div>

</section>
<section id="splash" class="slide level2" data-menu-title="Dancing Doggo">
<h2></h2>
<div class="quarto-layout-panel" data-fig-align="center">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="quarto-layout-cell" style="flex-basis: 46.9%;justify-content: center;">
<p><img data-src="assets/dog.gif"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 6.3%;justify-content: center;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 46.9%;justify-content: center;">
<p><img data-src="assets/music.jpg"></p>
</div>
</div>
</div>
<!-- Agenda -->
</section>
<section>
<section id="agenda" class="title-slide slide level1 center" data-menu-title="Agenda" data-background="#9EABAE" data-background-image="assets/lab.png" data-background-size="35%" data-background-position="right 5% top 5%">
<h1></h1>
<p><span class="r-fit-text">OUR AGENDA</span></p>
</section>
<section id="the-plan-for-today" class="slide level2">
<h2>The Plan for Today</h2>
<div class="columns">
<div class="column" style="width:65%;">
<ol type="1">
<li>Introduction</li>
<li>Terminology</li>
<li>Mediation (Postphenomenology)</li>
<li>Methods (Posthuman Inquiry)</li>
<li>Technology</li>
<li>Future</li>
</ol>
</div><div class="column" style="width:35%;">
<p>Link to this slide deck!</p>
<p></p><div id="qr1" class="qrcode"></div>
<script type="text/javascript">
var qrcode = new QRCode("qr1", {"height":"250","colorDark":"#000000","width":"250","colorLight":"#ffffff","text":"https://mavrxlab.org/news/2023-07-16-kti-c/deck/index.html"});
</script>
    <p></p>
</div>
</div>
<aside class="notes">
<p>The plan for today is to explore the basic concepts needed in order to really dig into how these technologies interact with and afford changes in not just education as a big, nebulous concept, but in our day-to-day. As we’re still very much in the early stages of this, much of it will be necessarily theoretical or look toward <em>potential</em> rather than a <em>postmortem</em>. That said, we’ll do our best to tease these things out and at the very least provide a roadmap.</p>
<p>Next we’ll be laying the groundwork for the ability to use that roadmap. We’ll start with the foundations of the various ways in which technology mediates our experience of the world. This is, in part, to give us a framework and language with which to understand new technologies like AI. This is known as postphenomenology.</p>
<p>After that, we’ll take that groundwork and look at methods of application. A kind of, “Okay, great, but <em>how</em>?” And, specifically, how <em>in education</em>? This is what we refer to as posthuman inquiry.</p>
<p>Next, we’ll look at what this potentially tells us about the future of this space. How can we make it successful? What do we need to be wary of?</p>
<p>Finally, I’ll share with you a toolkit for delving into this space for yourself. I’ve put together a lengthy collection of resources, platforms, and guides for you to use.</p>
<p>So, let’s get going!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- Introduction to speaker -->
</section></section>
<section>
<section id="who" class="title-slide slide level1 center" data-menu-title="Introduction" data-background="#9EABAE" data-background-image="assets/lab.png" data-background-size="35%" data-background-position="right 5% top 5%">
<h1></h1>
<p><span class="r-fit-text">INTRODUCTIONS</span></p>
</section>
<section id="ryan-straight" class="slide level2 smaller" data-menu-title="Ryan Straight">
<h2></h2>

<img data-src="assets/profile.jpg" class="r-stretch quarto-figure-center"><p><strong>Ryan Straight, Ph.D</strong><br>
Honors/Assistant Professor in Cyber, Intel, &amp; Information Operations<br>
Director, MA{VR}X Lab<br>
College of Applied Science and Technology<br>
University of Arizona</p>
<aside class="notes">
<ul>
<li>Me, what I teach, et cetera.</li>
<li>Going to be introducing you to a <em>lot</em> today, so you’ll leave with more questions than you probably came in with and I <em>think</em> that’s usually the better result.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- Terminology -->
</section></section>
<section>
<section id="terminology" class="title-slide slide level1 center" data-menu-title="Terminology" data-background="#9EABAE" data-background-image="assets/lab.png" data-background-size="35%" data-background-position="right 5% top 5%">
<h1></h1>
<p><span class="r-fit-text">TERMINOLOGY</span></p>
</section>
<section id="lets-speak-the-same-language" class="slide level2">
<h2>Let’s speak the same language</h2>
<div class="columns">
<div class="column" style="width:65%;">
<div>
<ol type="1">
<li class="fragment">Posthuman</li>
<li class="fragment">Posthuman Inquiry &amp; Postphenomenology</li>
<li class="fragment">Artificial Intelligence (AI)</li>
<li class="fragment">Virtual Pedagogical Agents</li>
</ol>
</div>
</div><div class="column" style="width:35%;">
<p><img data-src="assets/terminology.jpg"></p>
</div>
</div>
<div class="attribution">
<p>Image via <a href="https://dream.ai">dream.ai</a>.</p>
</div>
<aside class="notes">
<p>We need to be on the same page when we’re talking about these rather new topics, so I want to give a quick rundown of what these things actually are. I’ll caveat it with: so far, with just a few exceptions, there’s no absolute consensus here.</p>
<ul>
<li><strong>Posthuman</strong>: some think it means being a cyborg or living in The Matrix. Others see it as having the ability to alter one’s self just as easily as the material world around us. Brain-computer interfaces. Just about any episode of Black Mirror. But it’s not necessarily just like that. For our purposes, we’re focusing on understanding moving <em>beyond</em> the traditional human-technology dichotomy, which opens us up to actually understanding much, much more about how we now exist in the world.</li>
<li><strong>Posthuman Inquiry and Postphenomenology</strong>: these are technically separate approaches to researching in this space but in our brief time we’ll use the terms more or less interchangeably. They are both methodologies, ways to say “okay, so <strong>how</strong> do I go about understanding this new existence?” We’ll be coming back to this shortly.</li>
<li><strong>Artificial intelligence</strong>: our current iteration of AI is “generative,” meaning we have have so-called “large language models” that are used to generate responses to prompts. Anyone that’s used ChatGPT knows how this functions from a user-standpoint, if not perhaps the underlying technicalities.</li>
<li><strong>Virtual pedagogical agents</strong>: exactly what it sounds like and this is where things get <em>really</em> fun.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- Mediation -->
</section></section>
<section>
<section id="mediation" class="title-slide slide level1 center" data-menu-title="Mediation" data-background="#9EABAE" data-background-image="assets/lab.png" data-background-size="35%" data-background-position="right 5% top 5%">
<h1></h1>
<p><span class="r-fit-text">MEDIATION</span></p>
<aside class="notes">
<p>This brings us to “technological mediation.” What does that even mean? Some examples may shed light:</p>
<p>The Original Four</p>
<p>Don Ihde <span class="citation" data-cites="ihdeTechnologyLifeworldGarden1990">(<a href="#/references" role="doc-biblioref" onclick="">1990</a>)</span>, in his attempt to reconcile the “classical” phenomenology that came from the likes of Edmund Husserl, Maurice Merleau-Ponty, and Martin Heidegger with our technologically immersed existence, proposed a new version of phenomenology, a post-phenomenological approach. This “post” prefix is more akin to postmodernism rather than posthumous or postmortem, in that it refers to simply the next logical step rather than something that necessarily comes <em>after</em>. It is worth mentioning here that when Ihde was exploring this new paradigm, it was the 1970s, so consider “technology” (in all its forms discussed earlier) now and half a century ago.</p>
<p>Regardless, through his ponderings he decided upon four different “technics,” as he called them, or <em>ways technology mediate our experiences</em>. As he says, “Instruments are the means by which unspoken things ‘speak’, and unseen things become ‘visible.’” They are relational and include intention, which we’ll come back to in a minute. So, briefly, what are they?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="embodiment" class="slide level2 smaller">
<h2>Embodiment</h2>
<div class="columns">
<div class="column" style="width:50%;">
<blockquote>
<p>(human – technology) → world</p>
</blockquote>
<p>People and technology together relate to the world.</p>
<ul>
<li>You see <strong>through</strong> a telescope.</li>
<li>You talk <strong>through</strong> a phone.</li>
<li>There is technologic <strong>transparency</strong>.</li>
</ul>
</div><div class="column" style="width:50%;">
<div style="width:100%;height:0;padding-bottom:100%;position:relative;">
<iframe src="https://giphy.com/embed/gIxBtRsuxT0iWuBDY4" width="100%" height="100%" style="position:absolute" frameborder="0" class="giphy-embed" allowfullscreen="">
</iframe>
</div>
</div>
</div>
<div class="attribution">
<p>Image via <a href="https://giphy.com" target="_blank">GIPHY</a></p>
</div>
<aside class="notes">
<p>Here, the person more or less incorporates the technology into their outward, physical, proprioreceptory experience. A classic example is a blind cane. The cane, in essence, becomes an extension of themselves and their perception. Likewise, glasses or a telescope.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="hermeneutic" class="slide level2 smaller">
<h2>Hermeneutic</h2>
<div class="columns">
<div class="column" style="width:50%;">
<blockquote>
<p>human → (technology – world)</p>
</blockquote>
<ul>
<li>You <strong>read off</strong> a speedometer.</li>
<li>We <strong>interpret</strong> an x-ray.</li>
<li>We assume the translation is accurate.</li>
</ul>
</div><div class="column" style="width:50%;">
<div style="width:100%;height:0;padding-bottom:100%;position:relative;">
<iframe src="https://giphy.com/embed/zt0ZkDZXMEOS4" width="100%" height="100%" style="position:absolute" frameborder="0" class="giphy-embed" allowfullscreen="">
</iframe>
</div>
</div>
</div>
<div class="attribution">
<p>Image via <a href="https://giphy.com" target="_blank">GIPHY</a></p>
</div>
<aside class="notes">
<p>The hermeneutic relation, on the otherhand, involves <em>translating</em> or <em>interpreting</em> technology in order to understand the world. You essentially <em>read through</em> the artifact, such as a speedometer or a clock. Writing, itself, is a technology that falls within this category. (Kline, remember?)</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="alterity" class="slide level2 smaller">
<h2>Alterity</h2>
<div class="columns">
<div class="column" style="width:50%;">
<blockquote>
<p>human → technology (world)</p>
</blockquote>
<ul>
<li>Technology as <strong>other</strong>.</li>
<li>We’re in <strong>its</strong> system, not ours.</li>
<li>World <strong>withdraws</strong>; we focus on the <strong>technology</strong>.</li>
</ul>
</div><div class="column" style="width:50%;">
<div style="width:75%;height:0;padding-bottom:129%;position:relative;">
<iframe src="https://giphy.com/embed/VVAiHiDKeUHC0" width="100%" height="100%" style="position:absolute" frameborder="0" class="giphy-embed" allowfullscreen="">
</iframe>
</div>
</div>
</div>
<div class="attribution">
<p>Image via <a href="https://giphy.com" target="_blank">GIPHY</a></p>
</div>
<aside class="notes">
<p>In the alterity relation, the technological artifact is treated as itself, what Ihde deemed the “quasi-other.” This ranges from a blender to an Amazon Echo to a fully functioning robot.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="background" class="slide level2 smaller">
<h2>Background</h2>
<div class="columns">
<div class="column" style="width:50%;">
<blockquote>
<p>human → (technology / world)</p>
</blockquote>
<ul>
<li>Impacts our <strong>environment</strong>.</li>
<li>Through this, <strong>us</strong>.</li>
<li>Often don’t notice until it <strong>breaks</strong>.</li>
</ul>
</div><div class="column" style="width:50%;">
<div style="width:100%;height:0;padding-bottom:100%;position:relative;">
<iframe src="https://giphy.com/embed/Rf4SBc9erYPaLlOA0U" width="100%" height="100%" style="position:absolute" frameborder="0" class="giphy-embed" allowfullscreen="">
</iframe>
</div>
</div>
</div>
<div class="attribution">
<p>Image via <a href="https://giphy.com" target="_blank">GIPHY</a></p>
</div>
<aside class="notes">
<p>Finally, the background relation describes experiences like an air conditioner: this is technology that influences and impacts the world around you but you have little to no interaction with. It happens <em>in the background</em>, surprising enough.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="butwaitmore" class="title-slide slide level1 center" data-menu-title="But Wait, There's More" data-background="#9EABAE" data-background-image="assets/lab.png" data-background-size="35%" data-background-position="right 5% top 5%">
<h1></h1>
<p><span class="r-fit-text">BUT WAIT THERE’S MORE</span></p>
<aside class="notes">
<p>Later, other relations were beginning to be identified as technology changed and the <em>distance</em> and <em>intention</em> from and with us changed, as well. For example Verbeek <span class="citation" data-cites="verbeekCyborgIntentionalityRethinking2008">(<a href="#/references" role="doc-biblioref" onclick=""><strong>verbeekCyborgIntentionalityRethinking2008?</strong></a>)</span> describes a few new and necessary <em>hybrid intentionalities</em>:</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-next-generation" class="slide level2">
<h2>The Next Generation</h2>
<div class="columns">
<div class="column" style="width:70%;">
<h4 id="fusioncyborg"><span class="rn" data-rn-type="underline" data-rn-color="red" data-rn-index="1">Fusion/Cyborg</span></h4>
<blockquote>
<p>( human / technology ) → world</p>
</blockquote>
<h4 id="composite"><span class="rn" data-rn-type="underline" data-rn-color="red" data-rn-index="2">Composite</span></h4>
<blockquote>
<p>human → ( technology → world )</p>
</blockquote>
<h4 id="augmentation"><span class="rn" data-rn-type="underline" data-rn-color="red" data-rn-index="3">Augmentation</span></h4>
<blockquote>
<p>( human – technology ) → world <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;↘ ( technology – world )</p>
</blockquote>
</div><div class="column" style="width:30%;">
<p><img data-src="assets/nextgen.jpg"></p>
</div>
</div>
<aside class="notes">
<p>Here we start getting into more complex relations that you can imagine being used in cutting-edge or future online/virtual learning environments. Hopefully you’re beginning to see where I’m going with all this.</p>
<ul>
<li>First, <strong>fusion</strong> or a <strong>cyborg</strong> intentionality. Here we’re talking about everything from IUDs to pacemakers, from implanted RFID chips to cochlear implants. In this case, there is no physical separation of the self and the technology. In fact, removing one of them from the equation breaks the entire thing down. A pacemaker without a user has no impact, and a user without a pacermaker is… well. You get the point. The person and the technology are literally <strong>fused</strong> insofar as experiencing the world is concerned.</li>
<li><strong>Composite</strong> intentionality: <code>Human -&gt; ( Technology -&gt; World )</code>, wherein a person’s intention is directed toward the technology, and that technology’s <em>intention</em> is thereby directed at the world. While a thermometer <em>represents</em> the temperature in the hermeneutic relation, A thermal camera, for example, which translates a world we cannot naturally perceive (infrared radiation) into something we can (a colorful video) in the composite relation.</li>
<li><strong>Augmented</strong> intentionality: considerably more complex, it includes a kind of feedback loop, wherein the human’s intention directs the technology experience the world, the results of which are then fed back to the human and the cycle repeats itself. So, not only is the human experiencing the world through the technology, the human is also experiencing the technology’s experience of the world overlaid, and reacts to <em>that</em>.</li>
</ul>
<p>The question, then, is how many of these different kind of technological relations can you identify in this experience, alone, much less in the myriad “technologies” (remember Kline?) that make up Web3 in general.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- Posthuman inquiry methods -->
</section></section>
<section>
<section id="education" class="title-slide slide level1 center" data-menu-title="Education" data-background="#9EABAE" data-background-image="assets/lab.png" data-background-size="35%" data-background-position="right 5% top 5%">
<h1></h1>
<p><span class="r-fit-text">EDUCATION</span></p>
<aside class="notes">
<p>Bringing It All Together: Where Does Education Fit In?</p>
<p>So, while we can clearly see how, through a hermeneutic relation, a speedometer can mediate your experience of driving a car, for example, what happens when the goal of your interaction is <em>learning</em>?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="doing-postphenomenology" class="slide level2">
<h2>Doing Postphenomenology</h2>
<blockquote>
<p>… in education involves attending to the unique differences a particular technology makes to teaching practice, knowledge apprehension, and pedagogical meaning.</p>
</blockquote>
<aside class="notes">
<p>As Adams and Turville <span class="citation" data-cites="adamsDoingPostphenomenologyEducation2018">(<a href="#/references" role="doc-biblioref" onclick="">2018</a>)</span> point out, “Doing postphenomenology in education involves attending to the unique differences a particular technology makes to teaching practice, knowledge apprehension, and pedagogical meaning” (p.&nbsp;20). The question then becomes, in this new version of technologically-mediated learning <em>and of self</em>, what are these Web3 paradigms of teaching practice, knowledge apprehension, and pedagogical meaning?</p>
<p>Approaching learning in this way–that is, through a postphenomenological, posthuman inquiry lens–encourages us to scrape away at the outer encompassing veneer of the <em>digital self</em> and critically examine the delineation therein, harkening back to the traditional phenomenological “way in.” Crucially, <em>doing</em> postphenomenology has no actual, agreed-upon method <span class="citation" data-cites="rosenbergerFieldGuidePostphenomenology2015">(<a href="#/references" role="doc-biblioref" onclick=""><strong>rosenbergerFieldGuidePostphenomenology2015?</strong></a>)</span>. There are a variety of <em>suggested</em> heuristics <span class="citation" data-cites="adamsResearchingPosthumanWorld2016">(<a href="#/references" role="doc-biblioref" onclick=""><strong>adamsResearchingPosthumanWorld2016?</strong></a>)</span>, but no hard-and-fast checklist or guide.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="but-how" class="slide level2">
<h2>But How?</h2>
<div class="columns">
<div class="column" style="width:75%;">
<ol type="1">
<li class="fragment">Composing Anecdotes through Self-Observation</li>
<li class="fragment">Gathering Lived Experience Descriptions through Interviews</li>
<li class="fragment">Composing Anecdotes through Observation of Others</li>
<li class="fragment">Studying Breakdowns and the Eidetic Reduction</li>
</ol>
</div><div class="column" style="width:25%;">
<div style="width:100%;height:0;padding-bottom:83%;position:relative;">
<iframe src="https://giphy.com/embed/ounv1hey86r5DM6WhP" width="100%" height="100%" style="position:absolute" frameborder="0" class="giphy-embed" allowfullscreen="">
</iframe>
</div>
</div>
</div>
<div class="attribution">
<p>Image via <a href="https://giphy.com" target="_blank">GIPHY</a></p>
</div>
<aside class="notes">
<p>Accordingly, here are some ideas of ways you, too, can begin attempting to exploring this space and, if you spend time educating or being educated, the crossroads as well, as described by Adams and Turville.</p>
<ol type="1">
<li>Composing Anecdotes through Self-Observation
<ol type="1">
<li>Approach your own experiences and lifeworld with “non-intrusive observation of oneself” … and the “invitational appeals of one’s equipmental or technology-textured surround.”</li>
<li>This is harder than it sounds. “Explanations, opinions, judgments, or theoretical concepts must be pushed aside in favor of what was given in the moment.”</li>
</ol></li>
<li>Gathering Lived Experience Descriptions through Interviews
<ol type="1">
<li>Perhaps not the most easily accomplished approach, to be fair, especially for those that aren’t necessarily in an environment where that kind of research is encouraged or even permissible.</li>
<li>That said, this may be a tremendously revealing process, especially when directed toward teachers and learners in metaversal spaces.</li>
<li>This should <strong>not</strong> be confused with user experience testing, of course. When gathering these lived experience descriptions, while the design choices made by software developers may have an influential and mediating impact as discussed early, it is merely one step to overcome to get to the real experience.</li>
</ol></li>
<li>Composing Anecdotes through Observation of Others
<ol type="1">
<li>Contrasting the interview approach, simply observing others in the space can also lead to revelations without overt disturbance.</li>
<li>It may, for example, “assist in pointing up aspects of everyday life that may otherwise be taken for granted by oneself and others.”</li>
<li>However, it’s important to note that, since through the observation you may not access a truly sufficient and representative lived experience (you can imagine just how much you would miss by watching someone’s actions and not being privy to their motivations or goals).</li>
<li>This is the go-to when dealing with students, of course. Watch and learn.</li>
</ol></li>
<li>Finally, Studying Breakdowns and the Eidetic Reduction
<ol type="1">
<li>Recall Heidegger and his broken hammer. In this approach, we come <em>after</em>. That is, instead of trying to tease out the lived experiences <em>in vivo</em>, it is the breakdown of the moment that gives us access.</li>
<li>For example, when exploring how to understand AI in pedagogical contexts, it is easier to understand how people and things interact when there is friction than when it goes smoothly.</li>
<li>Likewise, for a completely non-technical and more Heideggerian example: when your pencil or chalk breaks mid-thought. What happens to you? What happens to your train of thought? What happens to the creation you were mid-process with?</li>
</ol></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="approaches" class="slide level2">
<h2>Approaches</h2>
<div class="columns">
<div class="column" style="width:70%;">
<ol type="1">
<li class="fragment">Variational method or analysis</li>
<li class="fragment">Variational cross-examination</li>
<li class="fragment">Case study</li>
<li class="fragment">Conversational analysis</li>
</ol>
</div><div class="column" style="width:30%;">
<div style="width:100%;height:0;padding-bottom:100%;position:relative;">
<iframe src="https://giphy.com/embed/2WUkAVDzuQbUA" width="100%" height="100%" style="position:absolute" frameborder="0" class="giphy-embed" allowfullscreen="">
</iframe>
</div>
</div>
</div>
<div class="attribution">
<p>Image via <a href="https://giphy.com" target="_blank">GIPHY</a></p>
</div>
<aside class="notes">
<p>We can make this even more concrete but first I need to briefly describe what postphenomenologists term “stabilities” and how technology is “multistable.”</p>
<p>…</p>
<ol type="1">
<li>Variational method or analysis: described as “the method of brainstorming stabilities of a multistable technology.” Remember, everything is also something else.</li>
<li>Variational cross-examination: first determining what a particular technology’s “dominant” stability is, we can move on to critically examining and identifying alternatives by exploring, for example, habits of users or frequent qualities teased out of the technology itself, its role in a particular system (thinking back to it being just an actor in a network of actors), and what we are probably most comfortable with or at least what we tend to do in this Web3 space: what’s known as “tailoring,” or “the … alterations of technology for different purposes.” Just think about how often we approach things in this space that way: “this has potential; how can we use it?”</li>
<li>Third, case study: generally speaking the postphenomenological approach is this. The Ed3DAO, for example, is a perfect candidate for this kind of approach, but so is something like the various platforms focusing on self-sovereign identity and data portability like Disco.</li>
<li>Finally, conversational analysis. This can be a little heady, but it can help us bridge the gap between simply exploring “what things do” and those mediations of our perceptions and actions “from within.”</li>
</ol>
<p>So, again, as you can see, there is no hard and fast, prescriptive method to take. This, perhaps surprisingly, works in our favor. It requires the educator and researcher to delve deep to fully understand something that they likely never will, only the individual instance of the thing. In doing so, however, incorporating the posthuman inquiry approach, we can interview these objects–whether tangible devices, virtual worlds, or digital personas–and give them time to “speak.” We musn’t simply explore how, for example, students “use” these things; we must conversely look at how those things use <em>them</em>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- AI -->
</section></section>
<section>
<section id="technology" class="title-slide slide level1 center" data-menu-title="The Technology" data-background="#9EABAE" data-background-image="assets/lab.png" data-background-size="35%" data-background-position="right 5% top 5%">
<h1></h1>
<p><span class="r-fit-text">THE TECHNOLOGY</span></p>
<aside class="notes">
<p>Now that we have some ways of actually doing these things, let’s try to bring it back to the technologies like Web3, XR, and artificial intelligence before we triangulate with education.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="web3-xr" class="slide level2">
<h2>Web3 &amp; XR</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/traditionalschooling.jpg" style="width:70.0%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/learninginfuture.jpg" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
<div class="attribution">

</div>
<aside class="notes">
<p>I want to touch to Web3 and XR quickly before moving on to artificial intelligence because I think they’ll both be providing a supporting role going forward. Let me explain.</p>
<p>Web3 is a catch-all term for, in short, a decentralized, blockchain-based internet. If you got a chance to check out Dagan’s talk from yesterday, he did a great job explaining it, so I won’t spend much time on it here. So, I’ll just reiterate that it’s meant to provide ownership and agency while increasing transparency, trust, and accessibility. My best advice when it comes to thinking about education and Web3 together is to divorce any thoughts of cryptocurrency from it and look solely and the utility of the technology. Okay, moving on.</p>
<p>XR, or extended reality, on the otherhand, has a much more intimate relationship with AI in the realm of pedagogy.(And this example is not intended to be abilist; extended reality is visual-heavy medium, however.) I’d like to paint a picture for you:</p>
<p>You’re looking at a student. They’re sitting in a chair, no desk in front of them, with their hands raised, moving as if directing an orchestra. Their eyes seem to be darting randomly and they repeatedly angle their head slightly in various directions. What do you imagine is going on?</p>
<p>In probably less time than you think, we won’t need bulky headsets to engage with that augmented layer of reality that we talked about earlier. Instead, simply contacts will be enough. Perhaps a couple rings to make hand tracking easier.</p>
<p>There’s still the question of what the learner is experiencing, though. Perhaps they’re manipulating a photo-realistic human heart. Or a relief valve in a nuclear reactor. Or unearthing an artifact in the Saqqara necropolis. All of these are possible but, as it stands now, they all must be pre-developed. Somewhere within the process, someone had to sit down and code this experience into life. Think about all the curriculum you’ve designed, after all, just on paper. If there’s narration or interactive instruction involved, doubly so. But what if that wasn’t the case?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="artifical-intelligence" class="slide level2">
<h2>Artifical Intelligence</h2>
<div class="columns">
<div class="column" style="width:65%;">
<ul>
<li>Fears</li>
<li>Accessibility</li>
<li>Ethics</li>
</ul>
</div><div class="column" style="width:35%;">
<p><img data-src="assets/ai.jpg"></p>
</div>
</div>
<aside class="notes">
<p>Remember, AI is not smart. I like the Microsoft leader that said, “Artificial Intelligence is neither artificial nor intelligent.” AI is just very confident but it’s only as good as the input, right?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- Future of AI in Education -->
</section></section>
<section>
<section id="future" class="title-slide slide level1 center" data-menu-title="The Future" data-background="#9EABAE" data-background-image="assets/lab.png" data-background-size="35%" data-background-position="right 5% top 5%">
<h1></h1>
<p><span class="r-fit-text">THE FUTURE</span></p>
<aside class="notes">
<p>The subtitle of this talk is “AI and the Future of Education,” but it might be more accurate to think of “Education and the Future of AI.”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="virtual-pedagogical-agents" class="slide level2">
<h2>Virtual Pedagogical Agents</h2>
<div class="columns">
<div class="column" style="width:50%;">
<h4 id="currently">Currently</h4>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/smartphone.jpg" style="width:65.0%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<h4 id="soon">Soon</h4>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/vpa.jpg" style="width:65.0%"></p>
</figure>
</div>
</div>
</div>
<div class="attribution">

</div>
<aside class="notes">
<p>This is where we are now. But:</p>
<ul>
<li>The current majority of traditionally paced students grew up with smartphones (I’m avoiding using terms like Gen Z as they’re just very poor categorizations and come with a LOT of baggage). A window to the world at all times.</li>
<li>Over the next decades, students in this same traditional pace will grow up with their own AIs. Like a valet. <span class="citation" data-cites="pittDoingPhilosophyTechnology2011">(<a href="#/references" role="doc-biblioref" onclick="">Pitt, 2011</a>)</span> “Someone” they grow with and that can push them constantly just outside their comfort zones while keeping the instructional approach and materials appropriate. Think of The Diamond Age with Nell and Miranda but without a live “ractor” on the other side.</li>
</ul>
<p>However, a question arises: well, a few, actually, that I invite folks to think about when we consider ways to both introduce AI to students <em>and</em> understand how it might impact them on an individualized level. I and a student just had a paper published in the EdMedia + Innovate Learning proceedings that relates to this, actually. <span class="citation" data-cites="straightParasocialPosthumanVirtual2023">(<a href="#/references" role="doc-biblioref" onclick="">Straight &amp; Yowika, 2023</a>)</span></p>
<p>But, even if we’re not at the AI place for this yet, it’s worth considering these questions for instructional design, alone:</p>
<ol type="1">
<li></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="preparing-students" class="slide level2">
<h2>Preparing Students</h2>
<div class="columns">
<div class="column" style="width:65%;">
<ol type="1">
<li>Support students <em>and</em> staff literacy</li>
<li>Staff well-equipped</li>
<li>Ethics first</li>
<li>Academic integrity</li>
<li>Best practices</li>
</ol>
</div><div class="column" style="width:35%;">
<p><img data-src="assets/studentprep.jpg"></p>
</div>
</div>
<div class="attribution">

</div>
<aside class="notes">
<p>As if students need us to prepare them for using AI. <em>Ethically</em> using it in a traditional educational setting, perhaps, yes. So, let’s look at what one group has suggested institutions do regarding the adoption of AI and ensuring proper student preparation.</p>
<p>But again, we’re still seeing this as reactive to the current state of AI, so is this enough? We’re in for probably one of the most difficult but exciting times since education has been a profession.</p>
<p>I think the most important thing here is ensuring that students are intrinsically motivated.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="our-posthuman-learners" class="slide level2">
<h2>Our Posthuman Learners(?)</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/phl1.jpg" style="width:65.0%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/phl2.jpg" style="width:65.0%"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>So, are our learners posthuman? If not yet, will they be?</p>
<p>If we believe that, yes, that certainly is or will be the case, we need these new ways of understanding the exploration and examination of the learner/technology hybrid that aren’t mired in traditional, perhaps out-moded approaches.</p>
<p>And, not to put too fine a point on it, but we’re hopefully in a position to realize that approaching AI and these other technologies going forward requires more than just some bullet points of action items: it becomes a philosophical discussion, an ethical discussion. Just think about the virtual pedagogical agent and compare a student’s lifelong AI with, for example, Deckard and Rachael in Blade Runner: should an AI persona be treated as human? What are the potential ethical consequences of learners treating an AI as a <em>teacher</em>, as a <em>collaborator</em>, or as a <em>servant</em>?</p>
<p>Either way, we absolutely need to start approaching our learners not simply as a student using a tool, but as the learner-AI hybrid entity and teach <em>to that</em>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="back-to-the-dancing-doggo" class="slide level2">
<h2>Back to the Dancing Doggo</h2>
<div class="quarto-layout-panel" data-fig-align="center">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="quarto-layout-cell" style="flex-basis: 46.9%;justify-content: center;">
<p><img data-src="assets/dog.gif"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 6.3%;justify-content: center;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 46.9%;justify-content: center;">
<p><img data-src="assets/music.jpg"></p>
</div>
</div>
</div>
<p>::: notes</p>
<p>Now back to the dancing doggo from the beginning and why it’s relevant for a talk on the future of AI and education.</p>
<p>Anyone want to hazard a guess as to what that music is on the right?</p>
<p>When Chopin saw a little dog dancing, he was inspired to write the Minute Waltz. It was the product of billions of neurological connections–some trained, some coincidental but serendipitous–that resulted in the music. It was <em>not</em> a well-engineered prompt. These connections are what drive creativity and what education feeds on.</p>
<!-- End slide -->
</section></section>
<section>
<section id="the-end" class="title-slide slide level1 center" data-menu-title="The End" data-background="#9EABAE" data-background-image="assets/lab.png" data-background-size="35%" data-background-position="right 5% top 5%">
<h1></h1>
<p><span class="r-fit-text">THE END</span></p>
</section>
<section id="thats-a-wrap" class="slide level2">
<h2>That’s a wrap!</h2>
<p>Thank you for coming! Find out more at:</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="mavrx-lab">MA{VR}X Lab</h3>
<p><a href="https://mavrxlab.org" class="uri">https://mavrxlab.org</a></p>
<h3 id="me">Me</h3>
<p><a href="https://ryanstraight.com" class="uri">https://ryanstraight.com</a></p>
</div><div class="column" style="width:50%;">
<h3 id="see-the-toolkit">See the toolkit!</h3>
<p><a href="https://mavrxlab.org/news/2023-07-16-kti-c/" class="uri">https://mavrxlab.org/news/2023-07-16-kti-c/</a></p>
<p></p><div id="qr2" class="qrcode"></div>
<script type="text/javascript">
var qrcode = new QRCode("qr2", {"height":"250","colorDark":"#000000","width":"250","colorLight":"#ffffff","text":"https://mavrxlab.org/news/2023-07-16-kti-c/"});
</script>
    <p></p>
</div>
</div>
<aside class="notes">
<p>Thanks, everybody, for coming out to hear these preliminary results from, as far as I can tell, the first and only study of its kind. I don’t want to give <em>everything</em> away so you’ll have something read when the paper drops!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="references" class="slide level2 scrollable smaller">
<h2>References</h2>

<img src="assets/lab.png" class="slide-logo r-stretch"><div class="footer footer-default">
<p>Dr.&nbsp;Ryan Straight – ryanstraight@arizona.edu – @ryanstraight@hci.social</p>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list">
<div id="ref-adamsDoingPostphenomenologyEducation2018" class="csl-entry" role="listitem">
Adams, C., &amp; Turville, J. (2018). Doing <span>Postphenomenology</span> in <span>Education</span>. In <em>Postphenomenological <span>Methodologies</span>: <span>New Ways</span> in <span>Mediating Techno-Human Relationships</span></em> (pp. 3–25). <span>Lexington Books</span>.
</div>
<div id="ref-ihdeTechnologyLifeworldGarden1990" class="csl-entry" role="listitem">
Ihde, D. (1990). <em>Technology and the <span>Lifeworld</span>: <span>From Garden</span> to <span>Earth</span></em>. <span>Indiana University Press</span>.
</div>
<div id="ref-pittDoingPhilosophyTechnology2011" class="csl-entry" role="listitem">
Pitt, J. C. (2011). <em>Doing <span>Philosophy</span> of <span>Technology</span></em> (Vol. 3). <span>Springer Netherlands</span>. <a href="https://doi.org/10.1007/978-94-007-0820-4">https://doi.org/10.1007/978-94-007-0820-4</a>
</div>
<div id="ref-straightParasocialPosthumanVirtual2023" class="csl-entry" role="listitem">
Straight, R., &amp; Yowika, W. (2023). From <span>Parasocial</span> to <span>Posthuman</span>: (<span>Virtual</span>) <span>Pedagogical Agents</span>, <span>Parasocial Phenomena</span>, and the <span>Future</span> of <span>Immersive Learning</span>. <em>Proceedings of <span>EdMedia</span> + <span>Innovate Learning</span></em>, 1028--1033. <a href="https://www.learntechlib.org/primary/p/222614/">https://www.learntechlib.org/primary/p/222614/</a>
</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="index_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="index_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="index_files/libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-attribution/attribution.js"></script>
  <script src="index_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="index_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="index_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="index_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="index_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"secret":"1689890960260561718","id":"e7ddc546d7aeb265","url":"https://reveal-multiplex.glitch.me/"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'print',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealAttribution, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>